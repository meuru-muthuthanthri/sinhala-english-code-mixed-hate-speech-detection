{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ponnayo danne kellek aduwa gaman laga inna kol...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ape harak samjeta eka honda adrshyak</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tpita pisuda yako man htuwe atta kiyala aiyo</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kimbak eduwoth ape untath amma thaththawath pe...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lisan nathawa yanna puluwan yako api dannawa o...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_hate\n",
       "0  ponnayo danne kellek aduwa gaman laga inna kol...     True\n",
       "1               ape harak samjeta eka honda adrshyak    False\n",
       "2       tpita pisuda yako man htuwe atta kiyala aiyo    False\n",
       "3  kimbak eduwoth ape untath amma thaththawath pe...     True\n",
       "4  lisan nathawa yanna puluwan yako api dannawa o...    False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "column_names = [\"text\", \"is_hate\"]\n",
    "\n",
    "df = pd.read_csv('1.preprocessed_data.csv', on_bad_lines='skip', sep=\",\", encoding='iso-8859-1', header=0, names=column_names)\n",
    "df['is_hate'] = df['is_hate'].astype(bool)\n",
    "df['text'] = df['text'].astype('str')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Data:\n",
    "Tokenize the text data to feed into DistilBert:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning DistilBert:\n",
    "Train the DistilBert model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/meuru/Projects/icbt/project/venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 0.6511623232343078\n",
      "Epoch: 1, Validation loss: 0.6212485879659653, Validation accuracy: 0.655328798185941\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.81      0.72       248\n",
      "           1       0.65      0.46      0.54       193\n",
      "\n",
      "    accuracy                           0.66       441\n",
      "   macro avg       0.65      0.63      0.63       441\n",
      "weighted avg       0.65      0.66      0.64       441\n",
      "\n",
      "Epoch: 2, Training loss: 0.5358924518478402\n",
      "Epoch: 2, Validation loss: 0.5302783149693694, Validation accuracy: 0.7528344671201814\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.69      0.76       248\n",
      "           1       0.68      0.83      0.75       193\n",
      "\n",
      "    accuracy                           0.75       441\n",
      "   macro avg       0.76      0.76      0.75       441\n",
      "weighted avg       0.77      0.75      0.75       441\n",
      "\n",
      "Epoch: 3, Training loss: 0.3749487867317588\n",
      "Epoch: 3, Validation loss: 0.4983245303322162, Validation accuracy: 0.7936507936507936\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83       248\n",
      "           1       0.82      0.68      0.74       193\n",
      "\n",
      "    accuracy                           0.79       441\n",
      "   macro avg       0.80      0.78      0.79       441\n",
      "weighted avg       0.80      0.79      0.79       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "# 1. Data Preparation\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def encode_text(df, tokenizer, max_length=256):\n",
    "    return tokenizer(df[\"text\"].tolist(), padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "encoded_data = encode_text(df, tokenizer)\n",
    "inputs, attention_masks, labels = encoded_data[\"input_ids\"], encoded_data[\"attention_mask\"], torch.tensor(df[\"is_hate\"].tolist()).long()\n",
    "\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    inputs, attention_masks, labels, test_size=0.2\n",
    ")\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=8)\n",
    "val_loader = DataLoader(val_data, batch_size=8)\n",
    "\n",
    "# 2. Model, Loss, and Optimizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "fpr = None\n",
    "tpr = None\n",
    "roc_auc = None\n",
    "cm_percentage = None\n",
    "\n",
    "\n",
    "# 3. Training Loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_mask)[0]\n",
    "        loss = loss_function(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Training loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation at the end of epoch\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    model_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_input_ids = batch[0].to(device)\n",
    "            batch_attention_mask = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)[0]\n",
    "            loss = loss_function(outputs, batch_labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions += (preds == batch_labels).sum().item()\n",
    "\n",
    "            true_labels.extend(batch_labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "            outputs_np = outputs.cpu().numpy()\n",
    "            model_outputs.extend(outputs_np)\n",
    "\n",
    "\n",
    "    val_accuracy = correct_predictions / len(val_labels)\n",
    "    print(f\"Epoch: {epoch + 1}, Validation loss: {total_val_loss / len(val_loader)}, Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "    print(report)\n",
    "\n",
    "    # values for confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # normalize the confusion matrix\n",
    "\n",
    "    # values for ROC curve\n",
    "    # Convert model output to probabilities and plot ROC curve\n",
    "    model_outputs = np.array(model_outputs)\n",
    "    probs = softmax(model_outputs, axis=1)[:, 1]  # Assume the second column is the probability for class \"1\"\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### took 13mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 'kalakanni deshapaluwo': Hate\n"
     ]
    }
   ],
   "source": [
    "def predict(model, tokenizer, device, text):\n",
    "    # Tokenize input string\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)[0]\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "input_string = \"kalakanni deshapaluwo\"\n",
    "model.eval()  # Ensure model is in evaluation mode\n",
    "prediction = predict(model, tokenizer, device, input_string)\n",
    "print(f\"Prediction for '{input_string}': {'Hate' if prediction == 1 else 'Not Hate'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "report =  classification_report(true_labels, predicted_labels, target_names=['True', 'False'])\n",
    "# save the values to a file\n",
    "with open('2.12 DistilBert.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc, 'cm_percentage': cm_percentage,'report': report\n",
    "    }, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
